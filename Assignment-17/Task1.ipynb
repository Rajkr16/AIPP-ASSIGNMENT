{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk pandas numpy scikit-learn beautifulsoup4 --quiet\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlv9rDNXYnlb",
        "outputId": "9883b90c-2eb4-410b-deea-746df8d1bcd4"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(\"\\n==================== TASK 1: SOCIAL MEDIA CLEANING ====================\")\n",
        "\n",
        "# Upload dataset in Google Colab\n",
        "#from google.colab import files\n",
        "#uploaded = files.upload()\n",
        "\n",
        "social_df = pd.read_csv(\"/content/social_media.csv\")\n",
        "\n",
        "# ----- BEFORE SUMMARY -----\n",
        "print(\"\\n--- BEFORE CLEANING ---\")\n",
        "print(social_df.head())\n",
        "\n",
        "# -------- REMOVE PUNCTUATION, STOPWORDS, SPECIAL SYMBOLS ----------\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)       # remove punctuation/symbols\n",
        "    words = [w for w in text.split() if w not in stop_words]\n",
        "    return \" \".join(words)\n",
        "\n",
        "social_df[\"clean_post\"] = social_df[\"post_text\"].apply(clean_text)\n",
        "\n",
        "# -------- HANDLE MISSING VALUES IN likes & shares ----------\n",
        "social_df[\"likes\"] = social_df[\"likes\"].fillna(social_df[\"likes\"].median())\n",
        "social_df[\"shares\"] = social_df[\"shares\"].fillna(0)\n",
        "\n",
        "# -------- TIMESTAMP PROCESSING ----------\n",
        "social_df[\"timestamp\"] = pd.to_datetime(social_df[\"timestamp\"])\n",
        "social_df[\"hour\"] = social_df[\"timestamp\"].dt.hour\n",
        "social_df[\"weekday\"] = social_df[\"timestamp\"].dt.day_name()\n",
        "\n",
        "# -------- REMOVE DUPLICATE / SPAM ----------\n",
        "social_df.drop_duplicates(subset=\"clean_post\", inplace=True)\n",
        "\n",
        "# ----- AFTER SUMMARY -----\n",
        "print(\"\\n--- AFTER CLEANING ---\")\n",
        "print(social_df.head())\n",
        "\n",
        "# -------- TEST CASES ----------\n",
        "assert social_df[\"clean_post\"].isna().sum() == 0\n",
        "assert social_df[\"likes\"].isna().sum() == 0\n",
        "assert \"hour\" in social_df.columns\n",
        "\n",
        "print(\"\\nTask 1 Passed All Tests ✔\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiQ2cDlVaIpI",
        "outputId": "894f82a1-0c55-4d92-e03c-3cf54f1a6891"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================== TASK 1: SOCIAL MEDIA CLEANING ====================\n",
            "\n",
            "--- BEFORE CLEANING ---\n",
            "   post_id    user                      post_text  likes  shares  \\\n",
            "0        1  user_1  This is a sample POST!!! #fun   20.0     1.0   \n",
            "1        2  user_2        <html>Great Day!</html>   20.0     3.0   \n",
            "2        3  user_3  This is a sample POST!!! #fun   20.0     1.0   \n",
            "3        4  user_4        <html>Great Day!</html>  100.0     NaN   \n",
            "4        5  user_5  This is a sample POST!!! #fun   20.0     5.0   \n",
            "\n",
            "             timestamp  \n",
            "0  2025-01-01 00:00:00  \n",
            "1  2025-01-01 06:00:00  \n",
            "2  2025-01-01 12:00:00  \n",
            "3  2025-01-01 18:00:00  \n",
            "4  2025-01-02 00:00:00  \n",
            "\n",
            "--- AFTER CLEANING ---\n",
            "   post_id    user                      post_text  likes  shares  \\\n",
            "0        1  user_1  This is a sample POST!!! #fun   20.0     1.0   \n",
            "1        2  user_2        <html>Great Day!</html>   20.0     3.0   \n",
            "\n",
            "            timestamp         clean_post  hour    weekday  \n",
            "0 2025-01-01 00:00:00    sample post fun     0  Wednesday  \n",
            "1 2025-01-01 06:00:00  htmlgreat dayhtml     6  Wednesday  \n",
            "\n",
            "Task 1 Passed All Tests ✔\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PssO2MGYaT0P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}